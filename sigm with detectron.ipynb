{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WZjcXJ93NNj-"},"outputs":[],"source":["import cv2\n","from PIL import ImageFont, ImageDraw, Image\n","import json\n","import os\n","from tqdm import tqdm\n","import numpy as np\n","import sys\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch, torchvision\n","import torch.nn as nn\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","\n","import logging"]},{"cell_type":"code","source":[],"metadata":{"id":"WC9hMRPPPaTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger('detectron2')\n","logger.setLevel(logging.CRITICAL)"],"metadata":{"id":"qzo3Nd8MNRJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EST_IMAGES_PATH, SAVE_FOLDER = sys.argv[1:]\n","\n","os.makedirs(SAVE_FOLDER, exist_ok=True)\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","SEGM_MODEL_PATH = 'segm-model_final.pth'\n","OCR_MODEL_PATH = 'ocr-model-last.ckpt'\n","\n","\n","CONFIG_JSON = {\n","    \"alphabet\": ''' !\"%\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№''',\n","    \"image\": {\n","        \"width\": 256,\n","        \"height\": 32\n","    }\n","}"],"metadata":{"id":"9rvCE4EdNSZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_contours_from_mask(mask, min_area=5):\n","    contours, hierarchy = cv2.findContours(mask.astype(np.uint8),\n","                                           cv2.RETR_LIST,\n","                                           cv2.CHAIN_APPROX_SIMPLE)\n","    contour_list = []\n","    for contour in contours:\n","        if cv2.contourArea(contour) >= min_area:\n","            contour_list.append(contour)\n","    return contour_list\n","\n","\n","def get_larger_contour(contours):\n","    larger_area = 0\n","    larger_contour = None\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area > larger_area:\n","            larger_contour = contour\n","            larger_area = area\n","    return larger_contour\n"],"metadata":{"id":"_nFOD1CMNU6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SEGMpredictor:\n","    def __init__(self, model_path):\n","        cfg = get_cfg()\n","        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","        cfg.MODEL.WEIGHTS = model_path\n","        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n","        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n","        cfg.INPUT.MIN_SIZE_TEST = 300\n","        cfg.INPUT.MAX_SIZE_TEST = 300\n","        cfg.INPUT.FORMAT = 'BGR'\n","        cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n","\n","        self.predictor = DefaultPredictor(cfg)\n","\n","    def __call__(self, img):\n","        outputs = self.predictor(img)\n","        prediction = outputs['instances'].pred_masks.cpu().numpy()\n","        contours = []\n","        for pred in prediction:\n","            contour_list = get_contours_from_mask(pred)\n","            contours.append(get_larger_contour(contour_list))\n","        return contours\n"],"metadata":{"id":"NokR0ggoNXzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OOV_TOKEN = '<OOV>'\n","CTC_BLANK = '<BLANK>'\n","\n","\n","def get_char_map(alphabet):\n","    \"\"\"Make from string alphabet character2int dict.\n","    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n","    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n","    char_map[CTC_BLANK] = 0\n","    char_map[OOV_TOKEN] = 1\n","    return char_map\n"],"metadata":{"id":"eSCo0CvDNoHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Tokenizer:\n","    \"\"\"Class for encoding and decoding string word to sequence of int\n","    (and vice versa) using alphabet.\"\"\"\n","\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","    def encode(self, word_list):\n","        \"\"\"Returns a list of encoded words (int).\"\"\"\n","        enc_words = []\n","        for word in word_list:\n","            enc_words.append(\n","                [self.char_map[char] if char in self.char_map\n","                 else self.char_map[OOV_TOKEN]\n","                 for char in word]\n","            )\n","        return enc_words\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n","        repeating characters. Also skip out of vocabulary token.\"\"\"\n","        dec_words = []\n","        for word in enc_word_list:\n","            word_chars = ''\n","            for idx, char_enc in enumerate(word):\n","\n","                if (\n","                    char_enc != self.char_map[OOV_TOKEN]\n","                    and char_enc != self.char_map[CTC_BLANK]\n","                    # idx > 0 to avoid selecting [-1] item\n","                    and not (idx > 0 and char_enc == word[idx - 1])\n","                ):\n","                    word_chars += self.rev_char_map[char_enc]\n","            dec_words.append(word_chars)\n","        return dec_words"],"metadata":{"id":"spvrrJB5NorG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Normalize:\n","    def __call__(self, img):\n","        img = img.astype(np.float32) / 255\n","        return img\n","\n","\n","class ToTensor:\n","    def __call__(self, arr):\n","        arr = torch.from_numpy(arr)\n","        return arr\n","\n","\n","class MoveChannels:\n","\n","\n","    def __init__(self, to_channels_first=True):\n","        self.to_channels_first = to_channels_first\n","\n","    def __call__(self, image):\n","        if self.to_channels_first:\n","            return np.moveaxis(image, -1, 0)\n","        else:\n","            return np.moveaxis(image, 0, -1)\n","\n","\n","class ImageResize:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, image):\n","        image = cv2.resize(image, (self.width, self.height),\n","                           interpolation=cv2.INTER_LINEAR)\n","        return image"],"metadata":{"id":"oSmDT_jXNqvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_val_transforms(height, width):\n","    transforms = torchvision.transforms.Compose([\n","        ImageResize(height, width),\n","        MoveChannels(to_channels_first=True),\n","        Normalize(),\n","        ToTensor()\n","    ])\n","    return transforms\n","\n","\n","def get_resnet34_backbone(pretrained=True):\n","    m = torchvision.models.resnet34(pretrained=True)\n","    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n","    blocks = [input_conv, m.bn1, m.relu,\n","              m.maxpool, m.layer1, m.layer2, m.layer3]\n","    return nn.Sequential(*blocks)\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size, hidden_size, num_layers,\n","            dropout=dropout, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        return out"],"metadata":{"id":"_orA-EE0Nuvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CRNN(nn.Module):\n","    def __init__(\n","        self, number_class_symbols, time_feature_count=256, lstm_hidden=256,\n","        lstm_len=2,\n","    ):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_backbone(pretrained=True)\n","        self.avg_pool = nn.AdaptiveAvgPool2d(\n","            (time_feature_count, time_feature_count))\n","        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden * 2, time_feature_count),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(time_feature_count, number_class_symbols)\n","        )\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x"],"metadata":{"id":"G_Y2rJSCN3dI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(images, model, tokenizer, device):\n","    model.eval()\n","    images = images.to(device)\n","    with torch.no_grad():\n","        output = model(images)\n","    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","    text_preds = tokenizer.decode(pred)\n","    return text_preds"],"metadata":{"id":"4zK6Cd3IN6fp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InferenceTransform:\n","    def __init__(self, height, width):\n","        self.transforms = get_val_transforms(height, width)\n","\n","    def __call__(self, images):\n","        transformed_images = []\n","        for image in images:\n","            image = self.transforms(image)\n","            transformed_images.append(image)\n","        transformed_tensor = torch.stack(transformed_images, 0)\n","        return transformed_tensor\n","\n","\n","class OcrPredictor:\n","    def __init__(self, model_path, config, device='cuda'):\n","        self.tokenizer = Tokenizer(config['alphabet'])\n","        self.device = torch.device(device)\n","\n","        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n","        self.model.load_state_dict(torch.load(model_path))\n","        self.model.to(self.device)\n","\n","        self.transforms = InferenceTransform(\n","            height=config['image']['height'],\n","            width=config['image']['width'],\n","        )\n","\n","    def __call__(self, images):\n","        if isinstance(images, (list, tuple)):\n","            one_image = False\n","        elif isinstance(images, np.ndarray):\n","            images = [images]\n","            one_image = True\n","        else:\n","            raise Exception(f\"Input must contain np.ndarray, \"\n","                            f\"tuple or list, found {type(images)}.\")\n","\n","        images = self.transforms(images)\n","        pred = predict(images, self.model, self.tokenizer, self.device)\n","\n","        if one_image:\n","            return pred[0]\n","        else:\n","            return pred"],"metadata":{"id":"wa6IAO8NN7gZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x2JC-dGoN-6b"},"execution_count":null,"outputs":[]}]}